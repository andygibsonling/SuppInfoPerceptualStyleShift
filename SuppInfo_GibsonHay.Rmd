---
title: "SupplementaryInfo_GibsonHay_LaS"
author: "Andy Gibson, Jen Hay"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Perceptual Style-shifting across Singing and Speech: Music activates Pop Song English for NZ Listeners

## Supplementary Materials
Andy Gibson and Jen Hay

## Contents
- This R Markdown file contains the code used for running the three models presented in the article, as well as for generating the plots.

1. Prerequisites
1. Read in Experiment 1 (PCT) data
1. Run Experiment 1 model
1. Plot Experiment 1 model predictions
1. Read in Experiment 2 (LDT) data
1. Run Experiment 2 accuracy model
1. Run Experiment 2 reaction time model
1. Plot Experiment 2 reaction time model predictions

## Prerequisites : load libraries
```{r load libraries, echo=T, message=F}
### load libraries ====
library(tidyverse) # data wrangling
library(lme4) # run models 
library(effects) # get model predictions
library(emmeans) # do pairwise analyses
library(lmerTest) # estimate p-values when summarising lmer models


# set up vif.mer function for later
vif.mer <- function (fit) {
  ## adapted from rms::vif
  
  v <- vcov(fit)
  nam <- names(fixef(fit))
  ## exclude intercepts
  ns <- sum(1 * (nam == "Intercept" | nam == "(Intercept)"))
  if (ns > 0) {
    v <- v[-(1:ns), -(1:ns), drop = FALSE]
    nam <- nam[-(1:ns)]
  }
  
  d <- diag(v)^0.5
  v <- diag(solve(v/(d %o% d)))
  names(v) <- nam
  v
}
```

## Read in Experiment 1 data

```{r read in PCT data}
#read.csv()


```

## Run Experiment 1 model

## Plot Experiment 1 model predictions


## Read in Experiment 2 (LDT) data

The data file should be in a folder called "Data" that is in the same location as this Rmd file.  Alternatively, you can change the path below yourself to find the file.

```{r read LDT data}
dat2 <- read.csv("Data/LDT_rawData.csv")
```

We need to remove outliers and nonwords, log and scale some variables, and create custom contrast coding

### Remove outliers and nonwords
```{r outlier removal}
# drop all tokens with rt < 400ms
dat2 %>% filter(RT >399) -> dat2 # 55 items removed

# drop all tokens where the reaction time is 3 by-speaker standard deviations
# above the by-speaker mean

# first get the mean and sd for each participant
dat2 %>% group_by(Participant) %>%summarise(meanRT = mean(RT), 
                                      sdRT = sd(RT)) -> means
# make the by-participant cut-offs outside 3 sds
means %>% mutate(topRT = meanRT + 3*sdRT) -> means
# merge the cutoffs back into the main data
left_join(dat2,means) -> dat2
# remove those where RT is above the cutoff
dat2 %>% filter(RT < topRT) -> dat2 # cut 174 tokens, from 10745 to 10571

# get rid of all nonwords
dat2 %>% filter(StimulusType == "Word") -> dat2 # down to 5308 trials
```

### Manipulate variables
```{r manipulate variables}
# log and scale RT, and the RT of the previous trial (PrevRT)
dat2$logRT <- log(dat2$RT)
# .lcs means logged, centred and scaled
dat2$RT.lcs <- scale(dat2$logRT)

dat2$logPrevRT <- log(dat2$PrevRT)
dat2$PrevRT.lcs <- scale(dat2$logPrevRT)
```

For this analysis, we are only interested in music vs. non music. so we can set up specific contrast coding.
This contrast coding method comes from [here](https://stats.oarc.ucla.edu/r/library/r-library-contrast-coding-systems-for-categorical-variables/#User) and 
[here](https://marissabarlaz.github.io/portfolio/contrastcoding/#:~:text=By%20default%2C%20a%20variable%20does,or%20define%20any%20contrast%20manually)

Some relevant quotes from the 2nd link:
Another example of contrast coding would be, say we had three groups: English, Spanish, and French speakers, who participated in a lexical decision task. If we wanted to, we could compare between English and non-English (i.e., Spanish and French), and then between Spanish and French speakers. The contrast for that would look like this:
```{r} 
matrix(c(1, -1/2, -1/2, 0, .5, -.5), ncol = 2)
```
The intercept would be the grand mean. The first coefficient the model would produce would be the difference between the intercept and English mean, and also would be twice the difference between the intercept and the average of Spanish and French means. The second coefficient would be the difference between the French and Spanish speakers, divided by 2.

```{r custom contrast coding}
# keep the original version with built-in treatment coding
dat2$ConditionTreatment <- as.factor(dat2$Condition)
# now make the version with custom contrasts
dat2$Condition <- as.factor(dat2$Condition)
mycontrasts2 = contrasts(dat2$Condition)
mycontrasts2[,1] = c(1, -1/2, -1/2)
mycontrasts2[,2] = c(0,.5,-.5)
colnames(mycontrasts2) <- c("Music_vs_NonMusic", "Noise_vs_Silence")
mycontrasts2
contrasts(dat2$Condition) = mycontrasts2
contrasts(dat2$Condition)
```
## Run Experiment 2 accuracy model



## Run Experiment 2 reaction time model

```{r run RT model 1}
LDT_RT_mod <- lmer(RT.lcs ~  Condition * Voice + PrevRT.lcs +
              (1|Participant) + (1 + Voice | Word), 
              data = dat2)
summary(LDT_RT_mod) 
# Condition and Voice interact: Music is different to non-music.
```

Run a version of the model with treatment coding and Music condition as reference level

```{r run RT model 2}

LDT_RT_mod_Treatment <- lmer(RT.lcs ~  ConditionTreatment * Voice + PrevRT.lcs +
              (1|Participant) + (1 + Voice | Word), 
              data = dat2)
summary(LDT_RT_mod_Treatment) 
##### Condition and Voice interact: Both the non-music conditions are significantly different to music.
```
Check the VIF on the model

```{r LDT vif}
vif.mer(LDT_RT_mod)
# max = 1.99
```


## Plot Experiment 2 reaction time model predictions

```{r Plot LDT}
##### PLOT IT!
effectsDF <- LDT_RT_mod %>% 
  Effect(c("Voice","Condition"), .) %>% 
  as.data.frame() 

# backtransform the RT and standard errors to milliseconds scale
effectsDF$RT <- effectsDF$fit * attr(dat2$RT.lcs, 'scaled:scale') + attr(dat2$RT.lcs, 'scaled:center')
effectsDF$RT <- exp(effectsDF$RT)
effectsDF$lowerRT <- effectsDF$lower * attr(dat2$RT.lcs, 'scaled:scale') + attr(dat2$RT.lcs, 'scaled:center')
effectsDF$lowerRT <- exp(effectsDF$lowerRT)
effectsDF$upperRT <- effectsDF$upper * attr(dat2$RT.lcs, 'scaled:scale') + attr(dat2$RT.lcs, 'scaled:center')
effectsDF$upperRT <- exp(effectsDF$upperRT)

# make a version of voice that let's us see the error bars better
effectsDF$VoiceN <- as.numeric(effectsDF$Voice)
effectsDF$VoiceN <- ifelse(effectsDF$Condition=="Music", effectsDF$VoiceN -.03, effectsDF$VoiceN)
effectsDF$VoiceN <- ifelse(effectsDF$Condition=="Noise", effectsDF$VoiceN +.03, effectsDF$VoiceN)

# PLOT IT
effectsDF  %>%  ggplot(aes(x=VoiceN, y=RT, shape=Condition, colour=Condition, linetype = Condition, group = Condition,  ymin=lowerRT, ymax=upperRT)) +
  geom_line() +
  geom_errorbar(width = 0.05, alpha=0.2) +
  geom_point(data = effectsDF, aes(x = Voice), size =0) +
  geom_point(data = effectsDF, aes(x = VoiceN)) +
  scale_y_continuous('Reaction time (ms)', n.breaks = 6) +
  scale_x_discrete('Speaker country of origin') +
  theme_bw(base_size=16) + theme(axis.text.y = element_text(size = 10)) +
  scale_colour_brewer(palette = 'Dark2')
# ggsave('LDT_Model_LaS_20231215.png', width=7, height = 6)

```


